# slurm.conf file generated by configurator.html.
# Put this file on all nodes of your cluster.
# See the slurm.conf man page for more information.
#
ControlMachine=<CONTROLLER HOSTNAME>
ControlAddr=<CONTROLLER HOSTNAME>
#BackupController=
#BackupAddr=
# 
AuthType=auth/none
#CheckpointType=checkpoint/none 
CommunicationParameters=NoAddrCache
CryptoType=crypto/munge
#DisableRootJobs=NO 
#EnforcePartLimits=NO 
#Epilog=
#EpilogSlurmctld= 
#FirstJobId=1 
#MaxJobId=999999 
#GresTypes= 
#GroupUpdateForce=0 
#GroupUpdateTime=600 
#JobCheckpointDir=/var/slurm/checkpoint 
#JobCredentialPrivateKey=
#JobCredentialPublicCertificate=
#JobFileAppend=0 
#JobRequeue=1 
#JobSubmitPlugins=1 
#KillOnBadExit=0 
#LaunchType=launch/slurm 
#Licenses=foo*4,bar 
MailProg=/bin/mail 
MaxJobCount=1000000
MaxArraySize=4000001
#MaxStepCount=40000 
#MaxTasksPerNode=128 
MpiDefault=none
#MpiParams=ports=#-# 
#PluginDir= 
#PlugStackConfig= 
#PrivateData=jobs 
ProctrackType=proctrack/cgroup
#Prolog=
#PrologFlags= 
#PrologSlurmctld= 
#PropagatePrioProcess=0 
#PropagateResourceLimits= 
#PropagateResourceLimitsExcept= 
#RebootProgram= 
ReturnToService=2
#SallocDefaultCommand= 
SlurmctldParameters=idle_on_node_suspend,cloud_dns # failed nodes get added back to the pool once they're deleted
SlurmctldPidFile=/var/run/slurmctld.pid
SlurmctldPort=6800-6817
SlurmdPidFile=/var/run/slurmd.pid
SlurmdPort=6818
SlurmdSpoolDir=/var/spool/slurm
SlurmUser=slurm
#SlurmdUser=root 
#SrunEpilog=
#SrunProlog=
StateSaveLocation=/var/spool/slurm
SwitchType=switch/none
#TaskEpilog=
TaskPlugin=task/cgroup
#TaskPluginParam=
#TaskProlog=
#TopologyPlugin=topology/tree 
#TmpFS=/tmp 
#TrackWCKey=no 
TreeWidth=65533
#UnkillableStepProgram= 
#UsePAM=0 

# 
# TIMERS 

#BatchStartTimeout=10 
#CompleteWait=0 
#EpilogMsgTime=2000 
#GetEnvTimeout=2 
#HealthCheckInterval=0 
#HealthCheckProgram= 
InactiveLimit=0
KillWait=300
#MessageTimeout=10 
#ResvOverRun=0 
MinJobAge=300
#OverTimeLimit=0 
SlurmctldTimeout=120
SlurmdTimeout=48
#UnkillableStepTimeout=60 
#VSizeFactor=0 
Waittime=0

# 
# SCHEDULING 

DefMemPerCPU=1024
#MaxMemPerCPU=0 
#SchedulerTimeSlice=30 
SchedulerType=sched/backfill
SchedulerParameters=default_gbytes,kill_invalid_depend
SelectType=select/cons_tres
SelectTypeParameters=CR_CPU_Memory

# 
# JOB PRIORITY 

#PriorityFlags= 
#PriorityType=priority/basic 
#PriorityDecayHalfLife= 
#PriorityCalcPeriod= 
#PriorityFavorSmall= 
#PriorityMaxAge= 
#PriorityUsageResetPeriod= 
#PriorityWeightAge= 
#PriorityWeightFairshare= 
#PriorityWeightJobSize= 
#PriorityWeightPartition= 
#PriorityWeightQOS= 

# 
# LOGGING AND ACCOUNTING 

#AccountingStorageEnforce=associations,limits,qos,safe
AccountingStorageHost=<CONTROLLER HOSTNAME>
#AccountingStorageLoc=
#AccountingStoragePass=
#AccountingStoragePort=
AccountingStorageType=accounting_storage/slurmdbd
#AccountingStorageUser=
ClusterName=cluster
#DebugFlags= 
#JobCompHost=
#JobCompLoc=
#JobCompPass=
#JobCompPort=
JobCompType=jobcomp/none
#JobCompUser=
#JobContainerType=job_container/none 
JobAcctGatherFrequency=30
JobAcctGatherType=jobacct_gather/linux
SlurmctldDebug=3
SlurmctldLogFile=/var/spool/slurm/slurm.log
SlurmdDebug=3
SlurmdLogFile=/var/spool/slurm/slurmd.log
#SlurmSchedLogFile= 
#SlurmSchedLogLevel= 

# 
# POWER SAVE SUPPORT FOR IDLE NODES (optional) 

SuspendProgram=/sgcpd/src/slurm_suspend_wrapper.sh
ResumeProgram=/sgcpd/src/slurm_resume_wrapper.sh
ResumeFailProgram=/sgcpd/src/slurm_suspend_wrapper.sh
SuspendTimeout=120
ResumeTimeout=180
ResumeRate=300
SuspendRate=100
SuspendTime=240

# 
# COMPUTE NODES 

NodeName=<WORKER HOSTNAME PREFIX>[1-2000] CPUs=8 RealMemory=28000 State=CLOUD 
PartitionName=gce_cluster Nodes=<WORKER HOSTNAME PREFIX>[1-2000] Default=YES MaxTime=INFINITE State=UP OverSubscribe=YES:10

# show all cloud nodes, even those that are nonexistent
PrivateData=cloud
